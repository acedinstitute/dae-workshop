{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Headlines\n",
    "\n",
    "This notebook shows you how to scrape news headlines from a specific news source over a longer period of time. As an example we'll use the website of the NOS (Nederlandse Omroep Stichting / Dutch Broadcast Foundation). They are a Dutch state-funded news organisation. Similar to the BBC or ORF etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from lxml import html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "from time import sleep\n",
    "from datetime import *\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a date range of where we want to start scrape from the archives and where we want to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_start_date = '01/01/2017'\n",
    "archive_end_date = '01/01/2020'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the date range using panda's `date_range` function and put the range into a list with propper date formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = [d.strftime('%Y-%m-%d') for d in pd.date_range(start=archive_start_date,end=archive_end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the list of dates to generate a list of urls that point to the archive pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_urls = [f\"https://www.nos.nl/nieuws/archief/{date}\" for date in date_range]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the list of URLs in a pandas DataFrame so we can manipulate it more easily and save it for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df = pd.DataFrame({'urls':date_urls})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the dataframe to a `csv` file. A text based format that spreadsheets like google sheets and excel can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df.to_csv('urls.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get individual rows by accessing a column and a row number. By splitting on `'/'` we get a list that has the individual elements in the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df['urls'][0].split(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can acces different parts of the list based on an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df['urls'][0].split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = seed_df['urls'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use Requests to get the HTML page from the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = req.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a tree that we can query in order to get information in a structured manner from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = html.fromstring(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the path that we got from the web inspector in order to get the titles, the headlines and the timestamps of the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = tree.xpath('//*[@id=\"archief\"]/ul/li/a/div[2]/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = tree.xpath('//*[@id=\"archief\"]/ul/li/a/div[1]/time/@datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then go over the list of urls, download the html files and extract the headlines, timestamps and urls. Finally we put those in a dataframe and save it to disk for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "max_count = len(date_urls) \n",
    "\n",
    "f = IntProgress(min=0, max=max_count, layout={'width':'auto'}) # instantiate the progress bar\n",
    "display(f) # display the bar\n",
    "\n",
    "while count < max_count:\n",
    "\n",
    "    cur_date = seed_df['urls'][count].split(\"/\")[-1] # current date of the archive-url\n",
    "    cur_url = seed_df['urls'][count] # current archive url\n",
    "\n",
    "    r = req.get(cur_url) #get the html\n",
    "\n",
    "#     with open(f'cache/nos.nl/{c}/{c}_{cur_date}.html', mode='wb') as localfile:\n",
    "#         localfile.write(r.content) # write retrieved html to cache\n",
    "\n",
    "    tree = html.fromstring(r.content)\n",
    "\n",
    "    urls = tree.xpath('//*[@id=\"archief\"]/ul/li/a/@href') # retrieve urls\n",
    "    timestamps = tree.xpath('//*[@id=\"archief\"]/ul/li/a/div[1]/time/@datetime') # retrieve timestamps as non-UTC Strings\n",
    "    titles = tree.xpath('//*[@id=\"archief\"]/ul/li/a/div[2]/text()') # retrieve article titles\n",
    "\n",
    "    urls = [f\"https://www.nos.nl{u}\" for u in urls] # create a list of article urls\n",
    "    df = pd.DataFrame({'timestamp':timestamps,'title':titles,'url':urls})\n",
    "    df.to_csv(f'../data/demo/nos.nl_{cur_date}.csv')\n",
    "\n",
    "    count += 1    \n",
    "\n",
    "    f.value = count # signal to increment the progress bar\n",
    "    f.description = f'[{count}/{max_count}]'\n",
    "\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
